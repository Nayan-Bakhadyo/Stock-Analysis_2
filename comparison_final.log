2025-12-01 11:23:12.482741: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro
2025-12-01 11:23:12.482768: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB
2025-12-01 11:23:12.482775: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.92 GB
2025-12-01 11:23:12.482802: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.
2025-12-01 11:23:12.482818: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)
2025-12-01 11:23:15.209017: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.
Traceback (most recent call last):
  File "/Users/Nayan/Documents/Business/Stock_Analysis/compare_lstm_xlstm_fixed.py", line 462, in <module>
    comparison = compare_models(
                 ^^^^^^^^^^^^^^^
  File "/Users/Nayan/Documents/Business/Stock_Analysis/compare_lstm_xlstm_fixed.py", line 348, in compare_models
    xlstm_results = train_xlstm_multi_horizon(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/Nayan/Documents/Business/Stock_Analysis/compare_lstm_xlstm_fixed.py", line 256, in train_xlstm_multi_horizon
    train_targets = torch.stack([y_train_dict[h] for h in horizons], dim=1)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: expected Tensor as element 0 in argument 0, but got numpy.ndarray

ERROR conda.cli.main_run:execute(49): `conda run python3 compare_lstm_xlstm_fixed.py PFL --horizons 1 3 5 10 15 21 --lstm-epochs 30 --xlstm-epochs 50 --xlstm-hidden 512 --xlstm-blocks 7` failed. (See above for error)

================================================================================
Using STANDARD single-point predictions
================================================================================


================================================================================
COMPARING LSTM vs xLSTM - PFL
================================================================================
Horizons: [1, 3, 5, 10, 15, 21]
LSTM epochs: 30
xLSTM epochs: 50, hidden: 512, blocks: 7
================================================================================

================================================================================
TRAINING LSTM (TensorFlow) - Multi-Horizon
================================================================================
âœ“ Loaded 600 days of data
  Date range: 2023-04-12 to 2025-11-27

--- Training LSTM for 1-day forecast ---
ğŸ“Š Preparing features...
ğŸ”„ Creating sequences (lookback=60, forecast=7 days)...
âœ“ Training set: 388 samples (each predicts 7 days)
âœ“ Validation set: 97 samples
ğŸ—ï¸ Building multi-output LSTM model (7-day predictions)...
ğŸš€ Training model...
âœ“ Training MAE: 0.101525
âœ“ Validation MAE: 0.102245
  âœ“ Predicted price: 384.00 (+0.00%)
    MAPE: 0.03%, Direction Acc: 0.0%

--- Training LSTM for 3-day forecast ---
ğŸ“Š Preparing features...
ğŸ”„ Creating sequences (lookback=60, forecast=7 days)...
âœ“ Training set: 388 samples (each predicts 7 days)
âœ“ Validation set: 97 samples
ğŸ—ï¸ Building multi-output LSTM model (7-day predictions)...
ğŸš€ Training model...
âœ“ Training MAE: 0.134645
âœ“ Validation MAE: 0.064569
  âœ“ Predicted price: 384.00 (+0.00%)
    MAPE: 0.02%, Direction Acc: 0.0%

--- Training LSTM for 5-day forecast ---
ğŸ“Š Preparing features...
ğŸ”„ Creating sequences (lookback=60, forecast=7 days)...
âœ“ Training set: 388 samples (each predicts 7 days)
âœ“ Validation set: 97 samples
ğŸ—ï¸ Building multi-output LSTM model (7-day predictions)...
ğŸš€ Training model...
âœ“ Training MAE: 0.166121
âœ“ Validation MAE: 0.072206
  âœ“ Predicted price: 384.00 (+0.00%)
    MAPE: 0.02%, Direction Acc: 0.0%

--- Training LSTM for 10-day forecast ---
ğŸ“Š Preparing features...
ğŸ”„ Creating sequences (lookback=60, forecast=7 days)...
âœ“ Training set: 388 samples (each predicts 7 days)
âœ“ Validation set: 97 samples
ğŸ—ï¸ Building multi-output LSTM model (7-day predictions)...
ğŸš€ Training model...
âœ“ Training MAE: 0.163819
âœ“ Validation MAE: 0.049303
  âœ“ Predicted price: 384.00 (+0.00%)
    MAPE: 0.01%, Direction Acc: 0.0%

--- Training LSTM for 15-day forecast ---
ğŸ“Š Preparing features...
ğŸ”„ Creating sequences (lookback=60, forecast=7 days)...
âœ“ Training set: 388 samples (each predicts 7 days)
âœ“ Validation set: 97 samples
ğŸ—ï¸ Building multi-output LSTM model (7-day predictions)...
ğŸš€ Training model...
âœ“ Training MAE: 0.142820
âœ“ Validation MAE: 0.059568
  âœ“ Predicted price: 384.00 (+0.00%)
    MAPE: 0.02%, Direction Acc: 0.0%

--- Training LSTM for 21-day forecast ---
ğŸ“Š Preparing features...
ğŸ”„ Creating sequences (lookback=60, forecast=7 days)...
âœ“ Training set: 388 samples (each predicts 7 days)
âœ“ Validation set: 97 samples
ğŸ—ï¸ Building multi-output LSTM model (7-day predictions)...
ğŸš€ Training model...
âœ“ Training MAE: 0.189976
âœ“ Validation MAE: 0.089241
  âœ“ Predicted price: 384.00 (+0.00%)
    MAPE: 0.02%, Direction Acc: 0.0%

âœ“ LSTM training completed in 122.37s

================================================================================
TRAINING xLSTM (PyTorch) - Multi-Horizon
================================================================================
âœ“ Loaded 900 days of data
  Date range: 2022-01-09 to 2025-11-27

âœ“ Data prepared
  Train samples: 631
  Test samples: 158
  Horizons: [1, 3, 5, 10, 15, 21]

âœ“ Using device: mps
Building xLSTM forecaster:
  Hidden size: 512
  Num blocks: 7
  Num heads: 8
  Horizons: [1, 3, 5, 10, 15, 21]
Building xLSTM forecaster:
  Hidden size: 512
  Num blocks: 7
  Num heads: 8
  Horizons: [1, 3, 5, 10, 15, 21]
âœ“ Model created - 12,482,678 parameters

================================================================================
Training for 50 epochs...
================================================================================


